# AI Data Storage Strategy - Gemini 3 Hybrid Architecture

**Status**: Planning Phase  
**Created**: 2024-11-19  
**Related**: AI_DATA_GENERATION_GEMINI_3_REFACTOR.md  
**Objective**: Design database storage for Gemini 3 generated data with dynamic column support

---

## Executive Summary

This document outlines the strategy for storing AI-generated data from the new Gemini 3 hybrid system into Supabase, with special focus on:

1. **Dynamic column creation** - Auto-create columns when users request new fields
2. **Hybrid storage model** - Fixed columns + JSONB for flexibility
3. **Metadata tracking** - Confidence scores, sources, enrichment methods
4. **Transaction safety** - Atomic operations with rollback support
5. **Type inference** - Smart column type detection from field names and values

### Key Design Decisions

‚úÖ **Auto-create columns** - New fields automatically become table columns  
‚úÖ **Hybrid storage** - Fixed columns (name, email, company) + JSONB data  
‚úÖ **Full traceability** - Track confidence, sources, enrichment method per field  
‚úÖ **Atomic transactions** - All-or-nothing inserts with proper error handling  
‚úÖ **Type inference** - Smart detection of email, phone, url, number, date types  

---

## Current Database Schema

### Tables Overview

**records** (User-facing data)
- Fixed columns: `id`, `table_id`, `organization_id`, `name`, `email`, `company`, `status`
- JSONB column: `data` - stores all custom fields
- Indexes: GIN index on `data`, full-text search on `search_vector`

**ai_generated_records** (Metadata)
- Tracks: `job_id`, `record_index`, `generated_data`, `sources`, `status`
- Missing: `field_confidence`, `enriched_fields`, `enrichment_method` (to be added)

**table_columns** (Schema definition)
- Defines: `name`, `label`, `type`, `options`, `is_required`, `display_order`
- Types: text, number, date, email, phone, select, multiselect, textarea, url, boolean

**ai_generation_jobs** (Job tracking)
- Tracks: `status`, `total_records`, `completed_records`, `failed_records`

---

## Database Migrations

### Migration 1: Enhance ai_generated_records ‚úÖ APPLIED

**Purpose**: Add Gemini 3 metadata fields


```sql
-- Add new columns for Gemini 3 metadata
ALTER TABLE ai_generated_records
ADD COLUMN field_confidence JSONB,
ADD COLUMN enriched_fields TEXT[],
ADD COLUMN enrichment_method TEXT CHECK (
  enrichment_method IN ('knowledge', 'url_context', 'firecrawl', 'hybrid')
);

-- Add indexes for performance
CREATE INDEX idx_ai_generated_records_enrichment_method 
ON ai_generated_records(enrichment_method);

CREATE INDEX idx_ai_generated_records_field_confidence 
ON ai_generated_records USING GIN (field_confidence);

-- Add comments
COMMENT ON COLUMN ai_generated_records.field_confidence IS 
  'JSONB map of field names to confidence scores (0-1)';
COMMENT ON COLUMN ai_generated_records.enriched_fields IS 
  'Array of field names that were enriched in Phase 2';
COMMENT ON COLUMN ai_generated_records.enrichment_method IS 
  'Primary enrichment method: knowledge, url_context, firecrawl, or hybrid';
```

**Example data:**
```json
{
  "field_confidence": {
    "company_name": 0.95,
    "website": 0.90,
    "email": 0.95,
    "employee_count": 0.85,
    "revenue": 0.70
  },
  "enriched_fields": ["email", "employee_count", "revenue"],
  "enrichment_method": "url_context"
}
```

---

### Migration 2: Add unique constraint to table_columns ‚úÖ APPLIED

**Purpose**: Prevent duplicate column names in same table

```sql
-- Add unique constraint
ALTER TABLE table_columns
ADD CONSTRAINT unique_table_column_name 
UNIQUE (table_id, name);

-- This enables ON CONFLICT DO NOTHING for idempotent column creation
```

---

### Migration 3: Track AI-generated records ‚úÖ APPLIED

**Purpose**: Link records back to AI generation jobs

```sql
-- Add tracking columns to records table
ALTER TABLE records
ADD COLUMN is_ai_generated BOOLEAN DEFAULT FALSE,
ADD COLUMN ai_generation_job_id UUID REFERENCES ai_generation_jobs(id);

-- Add index for filtering AI-generated records
CREATE INDEX idx_records_ai_generated 
ON records(is_ai_generated, ai_generation_job_id);

-- Add comment
COMMENT ON COLUMN records.is_ai_generated IS 
  'True if this record was generated by AI enrichment system';
```

**Benefits:**
- Easy filtering of AI vs manual records
- Ability to bulk delete/update AI-generated records
- Audit trail for data provenance

---

## Data Insertion Flow

### Overview

```
Gemini 3 Generation (Phase 1 + 2)
  ‚Üì
RecordInsertionService
  ‚îú‚îÄ Step 1: Analyze generated fields
  ‚îú‚îÄ Step 2: Create missing columns
  ‚îú‚îÄ Step 3: Map to records format
  ‚îú‚îÄ Step 4: Insert into records (transaction)
  ‚îú‚îÄ Step 5: Insert into ai_generated_records (transaction)
  ‚îî‚îÄ Step 6: Update job progress
```

---

### Step 1: Analyze Generated Fields

**Input**: Generated company data from Gemini 3

```typescript
interface GeneratedCompany {
  name: string;
  website: string;
  fields: Array<{
    name: string;
    value: any;
    confidence: number;
    source: 'knowledge' | 'url_context' | 'firecrawl';
    sourceUrl?: string;
  }>;
}
```

**Process**:
1. Get existing columns from `table_columns` for this table
2. Identify new fields not in existing columns
3. Infer types for new fields
4. Prepare column creation statements

**Code**:
```typescript
async analyzeFields(
  tableId: string,
  companies: GeneratedCompany[]
): Promise<FieldAnalysis> {
  // Get existing columns
  const existingColumns = await this.getTableColumns(tableId);
  const existingNames = new Set(existingColumns.map(c => c.name));
  
  // Find new fields across all companies
  const newFields = new Set<string>();
  for (const company of companies) {
    for (const field of company.fields) {
      if (!existingNames.has(field.name)) {
        newFields.add(field.name);
      }
    }
  }
  
  // Infer types for new fields
  const newColumns = Array.from(newFields).map(fieldName => ({
    name: fieldName,
    label: this.formatLabel(fieldName),
    type: this.inferType(fieldName, companies),
    display_order: existingColumns.length + newFields.size,
  }));
  
  return { existingColumns, newColumns };
}
```

---

### Step 2: Create Missing Columns

**Purpose**: Auto-create columns in `table_columns` for new fields

**SQL**:
```sql
INSERT INTO table_columns (
  table_id, 
  name, 
  label, 
  type, 
  display_order,
  created_at
)
VALUES ($1, $2, $3, $4, $5, NOW())
ON CONFLICT (table_id, name) DO NOTHING;
```

**Type Inference Logic**:
```typescript
inferType(fieldName: string, companies: GeneratedCompany[]): ColumnType {
  // Pattern-based inference
  if (fieldName.includes('email')) return 'email';
  if (fieldName.includes('phone') || fieldName.includes('tel')) return 'phone';
  if (fieldName.includes('url') || fieldName.includes('website')) return 'url';
  if (fieldName.includes('date') || fieldName.includes('founded')) return 'date';
  
  // Value-based inference (sample first non-null value)
  const sampleValue = this.getSampleValue(fieldName, companies);
  if (typeof sampleValue === 'number') return 'number';
  if (typeof sampleValue === 'boolean') return 'boolean';
  if (this.isDateString(sampleValue)) return 'date';
  if (this.isUrl(sampleValue)) return 'url';
  if (this.isEmail(sampleValue)) return 'email';
  
  // Default to text
  return 'text';
}
```

**Progress Message**:
```typescript
onProgress?.({
  stage: 'insertion',
  message: `Creating new columns: ${newColumns.map(c => c.name).join(', ')}`,
  type: 'info',
});
```

---

### Step 3: Map to Records Format

**Purpose**: Transform generated data into records table format

**Mapping Rules**:

1. **Fixed Columns**:
   - `name` ‚Üê `company_name` or `name` field
   - `email` ‚Üê `email` field
   - `company` ‚Üê `company_name` field (duplicate for compatibility)
   - `status` ‚Üê First status from `table_statuses` (default)

2. **JSONB Data Column**:
   - All other fields go into `data` JSONB column
   - Structure: `{"field_name": "value", ...}`

**Code**:
```typescript
mapToRecordFormat(
  company: GeneratedCompany,
  tableId: string,
  organizationId: string,
  userId: string,
  defaultStatus: string
): RecordInsert {
  const fieldMap = new Map(company.fields.map(f => [f.name, f.value]));
  
  // Extract fixed columns
  const name = fieldMap.get('company_name') || fieldMap.get('name') || company.name;
  const email = fieldMap.get('email') || null;
  const companyName = fieldMap.get('company_name') || name;
  
  // Build JSONB data (exclude fixed columns)
  const data: Record<string, any> = {};
  for (const field of company.fields) {
    if (!['company_name', 'name', 'email'].includes(field.name)) {
      data[field.name] = field.value;
    }
  }
  
  return {
    table_id: tableId,
    organization_id: organizationId,
    name,
    email,
    company: companyName,
    status: defaultStatus,
    data,
    created_by: userId,
    is_ai_generated: true,
  };
}
```

**Example**:

Input:
```json
{
  "name": "BASEÊ†™Âºè‰ºöÁ§æ",
  "website": "https://binc.jp",
  "fields": [
    { "name": "company_name", "value": "BASEÊ†™Âºè‰ºöÁ§æ", "confidence": 0.95 },
    { "name": "website", "value": "https://binc.jp", "confidence": 0.90 },
    { "name": "email", "value": "info@binc.jp", "confidence": 0.95 },
    { "name": "employee_count", "value": 150, "confidence": 0.85 },
    { "name": "revenue", "value": "ÈùûÂÖ¨Èñã", "confidence": 0.70 }
  ]
}
```

Output:
```sql
INSERT INTO records (
  table_id, organization_id, name, email, company, status, data, created_by, is_ai_generated
) VALUES (
  'uuid-123',
  'uuid-456',
  'BASEÊ†™Âºè‰ºöÁ§æ',
  'info@binc.jp',
  'BASEÊ†™Âºè‰ºöÁ§æ',
  'New',
  '{"website": "https://binc.jp", "employee_count": 150, "revenue": "ÈùûÂÖ¨Èñã"}'::jsonb,
  'uuid-789',
  true
);
```

---

### Step 4: Insert into Records (Transaction)

**Purpose**: Insert user-facing data atomically

**SQL**:
```sql
BEGIN;

-- Insert record
INSERT INTO records (
  table_id, 
  organization_id, 
  name, 
  email, 
  company, 
  status, 
  data, 
  created_by,
  is_ai_generated,
  ai_generation_job_id,
  created_at,
  updated_at
)
VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, NOW(), NOW())
RETURNING id;

COMMIT;
```

**Error Handling**:
```typescript
try {
  const { data, error } = await supabase
    .from('records')
    .insert(recordData)
    .select('id')
    .single();
  
  if (error) throw error;
  return data.id;
  
} catch (error) {
  // Rollback handled by Supabase
  onProgress?.({
    stage: 'insertion',
    message: `Failed to insert ${company.name}: ${error.message}`,
    type: 'error',
  });
  throw error;
}
```

**Progress Message**:
```typescript
onProgress?.({
  stage: 'insertion',
  message: `Inserted record ${index + 1}/${total}: ${company.name}`,
  type: 'success',
  company: company.name,
  progress: {
    current: index + 1,
    total,
    percentage: Math.round(((index + 1) / total) * 100),
  },
});
```

---

### Step 5: Insert into ai_generated_records (Transaction)

**Purpose**: Store metadata for traceability

**Build Metadata**:
```typescript
function buildMetadata(company: GeneratedCompany, recordIndex: number) {
  // Build field_confidence map
  const fieldConfidence: Record<string, number> = {};
  for (const field of company.fields) {
    fieldConfidence[field.name] = field.confidence;
  }
  
  // Build enriched_fields array (confidence < 0.80)
  const enrichedFields = company.fields
    .filter(f => f.confidence < 0.80 || f.source !== 'knowledge')
    .map(f => f.name);
  
  // Determine enrichment_method
  const sources = new Set(company.fields.map(f => f.source));
  let enrichmentMethod: string;
  if (sources.size === 1) {
    enrichmentMethod = Array.from(sources)[0];
  } else {
    enrichmentMethod = 'hybrid';
  }
  
  // Build sources array (existing format)
  const sourcesArray = company.fields.map(field => ({
    field: field.name,
    source: field.source,
    url: field.sourceUrl,
    confidence: field.confidence,
  }));
  
  return {
    field_confidence: fieldConfidence,
    enriched_fields: enrichedFields,
    enrichment_method: enrichmentMethod,
    sources: sourcesArray,
  };
}
```

**SQL**:
```sql
BEGIN;

INSERT INTO ai_generated_records (
  job_id,
  record_index,
  generated_data,
  sources,
  field_confidence,
  enriched_fields,
  enrichment_method,
  status,
  created_at,
  updated_at
)
VALUES ($1, $2, $3, $4, $5, $6, $7, 'success', NOW(), NOW())
RETURNING id;

COMMIT;
```

**Example Data**:
```json
{
  "job_id": "uuid-job-123",
  "record_index": 0,
  "generated_data": {
    "company_name": "BASEÊ†™Âºè‰ºöÁ§æ",
    "website": "https://binc.jp",
    "email": "info@binc.jp",
    "employee_count": 150,
    "revenue": "ÈùûÂÖ¨Èñã"
  },
  "sources": [
    {
      "field": "company_name",
      "source": "knowledge",
      "confidence": 0.95
    },
    {
      "field": "email",
      "source": "url_context",
      "url": "https://binc.jp/company",
      "confidence": 0.95
    }
  ],
  "field_confidence": {
    "company_name": 0.95,
    "website": 0.90,
    "email": 0.95,
    "employee_count": 0.85,
    "revenue": 0.70
  },
  "enriched_fields": ["email", "employee_count", "revenue"],
  "enrichment_method": "url_context"
}
```

---

### Step 6: Update Job Progress

**Purpose**: Track completion and update UI

**SQL**:
```sql
UPDATE ai_generation_jobs
SET 
  completed_records = completed_records + 1,
  updated_at = NOW()
WHERE id = $1;
```

**On Completion**:
```sql
UPDATE ai_generation_jobs
SET 
  status = 'completed',
  completed_at = NOW(),
  updated_at = NOW()
WHERE id = $1;
```

**On Error**:
```sql
UPDATE ai_generation_jobs
SET 
  failed_records = failed_records + 1,
  error_message = $2,
  updated_at = NOW()
WHERE id = $1;
```

---

## Service Implementation

### RecordInsertionService

**Location**: `Flowly/lib/services/enrichment/RecordInsertionService.ts`

**Interface**:
```typescript
interface RecordInsertionService {
  /**
   * Insert generated records into database
   * Handles column creation, data mapping, and metadata storage
   */
  insertGeneratedRecords(
    jobId: string,
    companies: GeneratedCompany[],
    tableId: string,
    organizationId: string,
    userId: string,
    onProgress?: ProgressCallback
  ): Promise<InsertionResult>;
  
  /**
   * Create missing columns in table_columns
   */
  createMissingColumns(
    tableId: string,
    newColumns: ColumnDefinition[]
  ): Promise<void>;
  
  /**
   * Map generated data to records table format
   */
  mapToRecordFormat(
    company: GeneratedCompany,
    tableId: string,
    organizationId: string,
    userId: string,
    defaultStatus: string
  ): RecordInsert;
  
  /**
   * Infer column type from field name and values
   */
  inferColumnType(
    fieldName: string,
    companies: GeneratedCompany[]
  ): ColumnType;
  
  /**
   * Build metadata for ai_generated_records
   */
  buildMetadata(
    company: GeneratedCompany,
    recordIndex: number
  ): RecordMetadata;
}
```

**Key Methods**:


```typescript
class RecordInsertionService {
  constructor(private supabase: SupabaseClient) {}
  
  async insertGeneratedRecords(
    jobId: string,
    companies: GeneratedCompany[],
    tableId: string,
    organizationId: string,
    userId: string,
    onProgress?: ProgressCallback
  ): Promise<InsertionResult> {
    
    // Step 1: Analyze fields
    onProgress?.({
      stage: 'insertion',
      message: 'Analyzing generated fields...',
      type: 'info',
    });
    
    const { existingColumns, newColumns } = await this.analyzeFields(
      tableId,
      companies
    );
    
    // Step 2: Create missing columns
    if (newColumns.length > 0) {
      onProgress?.({
        stage: 'insertion',
        message: `Creating ${newColumns.length} new columns: ${newColumns.map(c => c.name).join(', ')}`,
        type: 'info',
      });
      
      await this.createMissingColumns(tableId, newColumns);
    }
    
    // Get default status
    const defaultStatus = await this.getDefaultStatus(tableId);
    
    // Step 3-6: Insert each record
    const insertedRecords: string[] = [];
    const errors: Array<{ company: string; error: string }> = [];
    
    for (let i = 0; i < companies.length; i++) {
      const company = companies[i];
      
      try {
        // Map to record format
        const recordData = this.mapToRecordFormat(
          company,
          tableId,
          organizationId,
          userId,
          defaultStatus
        );
        
        // Insert into records
        const recordId = await this.insertRecord(recordData, jobId);
        
        // Build metadata
        const metadata = this.buildMetadata(company, i);
        
        // Insert into ai_generated_records
        await this.insertMetadata(jobId, i, company, metadata);
        
        // Update progress
        await this.updateJobProgress(jobId, 'completed');
        
        insertedRecords.push(recordId);
        
        onProgress?.({
          stage: 'insertion',
          message: `Inserted ${i + 1}/${companies.length}: ${company.name}`,
          type: 'success',
          company: company.name,
          progress: {
            current: i + 1,
            total: companies.length,
            percentage: Math.round(((i + 1) / companies.length) * 100),
          },
        });
        
      } catch (error) {
        errors.push({
          company: company.name,
          error: error.message,
        });
        
        await this.updateJobProgress(jobId, 'failed', error.message);
        
        onProgress?.({
          stage: 'insertion',
          message: `Failed to insert ${company.name}: ${error.message}`,
          type: 'error',
          company: company.name,
        });
      }
    }
    
    // Final status update
    if (errors.length === 0) {
      await this.completeJob(jobId);
      onProgress?.({
        stage: 'complete',
        message: `All ${companies.length} records inserted successfully`,
        type: 'success',
      });
    } else {
      onProgress?.({
        stage: 'complete',
        message: `Inserted ${insertedRecords.length}/${companies.length} records (${errors.length} failed)`,
        type: 'warning',
      });
    }
    
    return {
      success: insertedRecords.length,
      failed: errors.length,
      insertedRecords,
      errors,
    };
  }
  
  private async insertRecord(
    recordData: RecordInsert,
    jobId: string
  ): Promise<string> {
    const { data, error } = await this.supabase
      .from('records')
      .insert({
        ...recordData,
        ai_generation_job_id: jobId,
      })
      .select('id')
      .single();
    
    if (error) throw new Error(`Failed to insert record: ${error.message}`);
    return data.id;
  }
  
  private async insertMetadata(
    jobId: string,
    recordIndex: number,
    company: GeneratedCompany,
    metadata: RecordMetadata
  ): Promise<void> {
    const { error } = await this.supabase
      .from('ai_generated_records')
      .insert({
        job_id: jobId,
        record_index: recordIndex,
        generated_data: this.buildGeneratedData(company),
        sources: metadata.sources,
        field_confidence: metadata.field_confidence,
        enriched_fields: metadata.enriched_fields,
        enrichment_method: metadata.enrichment_method,
        status: 'success',
      });
    
    if (error) throw new Error(`Failed to insert metadata: ${error.message}`);
  }
  
  private buildGeneratedData(company: GeneratedCompany): Record<string, any> {
    const data: Record<string, any> = {};
    for (const field of company.fields) {
      data[field.name] = field.value;
    }
    return data;
  }
}
```

---

## Edge Cases & Error Handling

### Edge Case 1: Duplicate Column Names

**Scenario**: Column "email" already exists, AI generates "email" field

**Solution**:
```sql
INSERT INTO table_columns (table_id, name, label, type, display_order)
VALUES ($1, $2, $3, $4, $5)
ON CONFLICT (table_id, name) DO NOTHING;
```

**Result**: Existing column is preserved, no error thrown

---

### Edge Case 2: Type Mismatch

**Scenario**: Column "employee_count" exists as `text`, AI generates `number` value

**Solution**: Coerce to string when inserting
```typescript
function coerceValue(value: any, columnType: ColumnType): any {
  if (value === null || value === undefined) return null;
  
  switch (columnType) {
    case 'text':
    case 'textarea':
      return String(value);
    case 'number':
      return Number(value);
    case 'boolean':
      return Boolean(value);
    case 'date':
      return new Date(value).toISOString();
    default:
      return value;
  }
}
```

**Alternative**: Update column type if all existing values are compatible
```sql
-- Check if safe to change type
SELECT COUNT(*) FROM records 
WHERE table_id = $1 
AND data->>'employee_count' IS NOT NULL
AND data->>'employee_count' !~ '^[0-9]+$';

-- If count = 0, safe to update
UPDATE table_columns
SET type = 'number'
WHERE table_id = $1 AND name = 'employee_count';
```

---

### Edge Case 3: Required Fields Missing

**Scenario**: Column marked `is_required = true`, AI didn't generate that field

**Solution**: Set to null and log warning
```typescript
if (column.is_required && !fieldMap.has(column.name)) {
  onProgress?.({
    stage: 'insertion',
    message: `Warning: Required field "${column.name}" missing for ${company.name}`,
    type: 'warning',
    company: company.name,
  });
  
  // Set to null or default value
  data[column.name] = column.default_value || null;
}
```

---

### Edge Case 4: Fixed Column Mapping Conflicts

**Scenario**: AI generates both "name" and "company_name"

**Solution**: Prioritize "company_name"
```typescript
const name = fieldMap.get('company_name') 
  || fieldMap.get('name') 
  || company.name 
  || 'Unnamed Company';
```

**Priority Order**:
1. `company_name` field
2. `name` field
3. Company object name
4. Default: "Unnamed Company"

---

### Edge Case 5: JSONB Size Limits

**Scenario**: Generated data is very large (>1MB)

**Solution**: Truncate long text fields
```typescript
function truncateData(data: Record<string, any>, maxSize: number = 1_000_000): Record<string, any> {
  const json = JSON.stringify(data);
  
  if (json.length <= maxSize) return data;
  
  // Truncate long text fields
  const truncated = { ...data };
  for (const [key, value] of Object.entries(truncated)) {
    if (typeof value === 'string' && value.length > 10000) {
      truncated[key] = value.substring(0, 10000) + '... [truncated]';
    }
  }
  
  return truncated;
}
```

**Log Warning**:
```typescript
onProgress?.({
  stage: 'insertion',
  message: `Warning: Data truncated for ${company.name} (exceeded 1MB)`,
  type: 'warning',
  company: company.name,
});
```

---

### Edge Case 6: Invalid Data Types

**Scenario**: Email field contains non-email value

**Solution**: Store as-is, let validation happen in UI layer
```typescript
// No validation during insertion
// UI will show validation errors when user edits
data[field.name] = field.value;
```

**Rationale**:
- AI might generate partial/invalid data
- Better to store and let user fix than reject
- UI validation is more flexible

---

### Edge Case 7: Transaction Failures

**Scenario**: Database connection lost during insertion

**Solution**: Rollback and retry
```typescript
async insertWithRetry(
  recordData: RecordInsert,
  maxRetries: number = 3
): Promise<string> {
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      return await this.insertRecord(recordData);
    } catch (error) {
      if (attempt === maxRetries) throw error;
      
      // Exponential backoff
      await new Promise(resolve => 
        setTimeout(resolve, Math.pow(2, attempt) * 1000)
      );
    }
  }
}
```

---

### Edge Case 8: Concurrent Column Creation

**Scenario**: Multiple jobs try to create same column simultaneously

**Solution**: Use `ON CONFLICT DO NOTHING`
```sql
INSERT INTO table_columns (table_id, name, label, type, display_order)
VALUES ($1, $2, $3, $4, $5)
ON CONFLICT (table_id, name) DO NOTHING;
```

**Result**: First insert wins, others are ignored (no error)

---

## Integration with DataGenerationOrchestrator

### Updated Flow

```typescript
class DataGenerationOrchestrator {
  constructor(
    private gemini3Service: Gemini3Service,
    private knowledgeAgent: Gemini3KnowledgeAgent,
    private enrichmentAgent: Gemini3EnrichmentAgent,
    private insertionService: RecordInsertionService  // NEW
  ) {}
  
  async generateRecordsBatch(
    jobId: string,
    count: number,
    dataType: string,
    specifications: string | undefined,
    fields: EnrichmentField[],
    tableId: string,
    organizationId: string,
    userId: string,
    onProgress?: ProgressCallback
  ): Promise<InsertionResult> {
    
    // Phase 1: Knowledge Extraction (5-10s)
    const companies = await this.knowledgeAgent.execute(
      dataType,
      specifications,
      count,
      fields,
      onProgress
    );
    
    // Phase 2: Targeted Enrichment (20-40s)
    const enrichedCompanies = [];
    for (const company of companies) {
      const enriched = await this.enrichmentAgent.execute(
        company,
        onProgress
      );
      enrichedCompanies.push(enriched);
    }
    
    // Phase 3: Database Insertion (5-10s) - NEW!
    const result = await this.insertionService.insertGeneratedRecords(
      jobId,
      enrichedCompanies,
      tableId,
      organizationId,
      userId,
      onProgress
    );
    
    return result;
  }
}
```

---

## API Endpoint Updates

### POST /api/enrichment/generate

**Updated Response**:
```typescript
{
  "jobId": "uuid-123",
  "status": "processing",
  "message": "Generation started",
  "sessionId": "session-456"  // For SSE progress
}
```

**Updated Flow**:
1. Create `ai_generation_jobs` record
2. Call `DataGenerationOrchestrator.generateRecordsBatch()`
3. Stream progress via SSE
4. Return final result

---

### GET /api/enrichment/progress/[sessionId]

**New Progress Stages**:
```typescript
type ProgressStage = 
  | 'knowledge_extraction'  // Phase 1
  | 'enrichment'            // Phase 2
  | 'insertion'             // Phase 3 (NEW!)
  | 'complete';
```

**New Progress Messages**:
```typescript
// Insertion stage
{
  stage: 'insertion',
  message: 'Analyzing generated fields...',
  type: 'info'
}

{
  stage: 'insertion',
  message: 'Creating 3 new columns: employee_count, revenue, gmv',
  type: 'info'
}

{
  stage: 'insertion',
  message: 'Inserted 1/5: BASEÊ†™Âºè‰ºöÁ§æ',
  type: 'success',
  company: 'BASEÊ†™Âºè‰ºöÁ§æ',
  progress: { current: 1, total: 5, percentage: 20 }
}

{
  stage: 'insertion',
  message: 'Failed to insert STORESÊ†™Âºè‰ºöÁ§æ: Database connection lost',
  type: 'error',
  company: 'STORESÊ†™Âºè‰ºöÁ§æ'
}

{
  stage: 'complete',
  message: 'All 5 records inserted successfully',
  type: 'success'
}
```

---

## Testing Strategy

### Unit Tests

**Test 1: Type Inference**
```typescript
describe('RecordInsertionService.inferColumnType', () => {
  it('should infer email type from field name', () => {
    const type = service.inferColumnType('contact_email', companies);
    expect(type).toBe('email');
  });
  
  it('should infer number type from value', () => {
    const type = service.inferColumnType('employee_count', companies);
    expect(type).toBe('number');
  });
  
  it('should default to text for unknown types', () => {
    const type = service.inferColumnType('custom_field', companies);
    expect(type).toBe('text');
  });
});
```

**Test 2: Field Mapping**
```typescript
describe('RecordInsertionService.mapToRecordFormat', () => {
  it('should map company_name to both name and company', () => {
    const record = service.mapToRecordFormat(company, tableId, orgId, userId, 'New');
    expect(record.name).toBe('BASEÊ†™Âºè‰ºöÁ§æ');
    expect(record.company).toBe('BASEÊ†™Âºè‰ºöÁ§æ');
  });
  
  it('should put non-fixed fields in data JSONB', () => {
    const record = service.mapToRecordFormat(company, tableId, orgId, userId, 'New');
    expect(record.data).toHaveProperty('website');
    expect(record.data).toHaveProperty('employee_count');
  });
});
```

**Test 3: Metadata Building**
```typescript
describe('RecordInsertionService.buildMetadata', () => {
  it('should build field_confidence map', () => {
    const metadata = service.buildMetadata(company, 0);
    expect(metadata.field_confidence).toHaveProperty('company_name', 0.95);
    expect(metadata.field_confidence).toHaveProperty('email', 0.95);
  });
  
  it('should identify enriched fields', () => {
    const metadata = service.buildMetadata(company, 0);
    expect(metadata.enriched_fields).toContain('email');
    expect(metadata.enriched_fields).toContain('employee_count');
  });
  
  it('should determine enrichment method', () => {
    const metadata = service.buildMetadata(company, 0);
    expect(metadata.enrichment_method).toBe('url_context');
  });
});
```

---

### Integration Tests

**Test 1: End-to-End Insertion**
```typescript
describe('RecordInsertionService.insertGeneratedRecords', () => {
  it('should insert all records successfully', async () => {
    const result = await service.insertGeneratedRecords(
      jobId,
      companies,
      tableId,
      organizationId,
      userId
    );
    
    expect(result.success).toBe(5);
    expect(result.failed).toBe(0);
    expect(result.insertedRecords).toHaveLength(5);
  });
  
  it('should create missing columns', async () => {
    await service.insertGeneratedRecords(jobId, companies, tableId, orgId, userId);
    
    const columns = await supabase
      .from('table_columns')
      .select('name')
      .eq('table_id', tableId);
    
    expect(columns.data.map(c => c.name)).toContain('employee_count');
    expect(columns.data.map(c => c.name)).toContain('revenue');
  });
  
  it('should store metadata correctly', async () => {
    await service.insertGeneratedRecords(jobId, companies, tableId, orgId, userId);
    
    const metadata = await supabase
      .from('ai_generated_records')
      .select('*')
      .eq('job_id', jobId);
    
    expect(metadata.data).toHaveLength(5);
    expect(metadata.data[0]).toHaveProperty('field_confidence');
    expect(metadata.data[0]).toHaveProperty('enriched_fields');
    expect(metadata.data[0]).toHaveProperty('enrichment_method');
  });
});
```

**Test 2: Error Handling**
```typescript
describe('RecordInsertionService error handling', () => {
  it('should handle database connection errors', async () => {
    // Mock connection failure
    jest.spyOn(supabase, 'from').mockRejectedValueOnce(
      new Error('Connection lost')
    );
    
    const result = await service.insertGeneratedRecords(
      jobId, companies, tableId, orgId, userId
    );
    
    expect(result.failed).toBeGreaterThan(0);
    expect(result.errors[0].error).toContain('Connection lost');
  });
  
  it('should update job status on failure', async () => {
    jest.spyOn(supabase, 'from').mockRejectedValueOnce(
      new Error('Insert failed')
    );
    
    await service.insertGeneratedRecords(jobId, companies, tableId, orgId, userId);
    
    const job = await supabase
      .from('ai_generation_jobs')
      .select('failed_records')
      .eq('id', jobId)
      .single();
    
    expect(job.data.failed_records).toBeGreaterThan(0);
  });
});
```

---

## Performance Considerations

### Batch Inserts

**Current**: Insert one record at a time
```typescript
for (const company of companies) {
  await this.insertRecord(company);
}
```

**Optimized**: Batch insert multiple records
```typescript
const recordsData = companies.map(c => this.mapToRecordFormat(c, ...));

const { data, error } = await this.supabase
  .from('records')
  .insert(recordsData)
  .select('id');
```

**Benefits**:
- Reduce network round trips
- Faster insertion (5-10x)
- Lower latency

**Trade-offs**:
- Harder to track individual failures
- All-or-nothing (one failure = all fail)

**Recommendation**: Use batch inserts with error recovery
```typescript
try {
  // Try batch insert first
  await this.batchInsert(recordsData);
} catch (error) {
  // Fall back to individual inserts
  for (const record of recordsData) {
    try {
      await this.insertRecord(record);
    } catch (err) {
      // Log individual failure
    }
  }
}
```

---

### Index Optimization

**Existing Indexes**:
- GIN index on `records.data` (JSONB)
- Full-text search on `records.search_vector`

**New Indexes** (from migrations):
- `idx_ai_generated_records_enrichment_method`
- `idx_ai_generated_records_field_confidence` (GIN)
- `idx_records_ai_generated`

**Query Performance**:
```sql
-- Fast: Uses GIN index
SELECT * FROM records 
WHERE data @> '{"employee_count": 150}';

-- Fast: Uses btree index
SELECT * FROM records 
WHERE is_ai_generated = true;

-- Fast: Uses GIN index
SELECT * FROM ai_generated_records 
WHERE field_confidence @> '{"email": 0.95}';
```

---

### Memory Management

**Large Batches** (100+ records):
- Process in chunks of 10-20 records
- Release memory between chunks
- Use streaming for progress updates

```typescript
async insertLargeBatch(
  companies: GeneratedCompany[],
  chunkSize: number = 20
): Promise<InsertionResult> {
  const chunks = this.chunkArray(companies, chunkSize);
  
  for (const chunk of chunks) {
    await this.insertGeneratedRecords(jobId, chunk, ...);
    
    // Release memory
    if (global.gc) global.gc();
  }
}
```

---

## Migration Execution Plan

### Step 1: Apply Database Migrations ‚úÖ COMPLETE

**Migration 1**: Add metadata columns ‚úÖ
**Migration 2**: Add unique constraint ‚úÖ
**Migration 3**: Add AI tracking columns ‚úÖ

**Verified**:
- `ai_generated_records.field_confidence` (JSONB)
- `ai_generated_records.enriched_fields` (TEXT[])
- `ai_generated_records.enrichment_method` (TEXT with CHECK constraint)
- `records.is_ai_generated` (BOOLEAN, default false)
- `records.ai_generation_job_id` (UUID, FK to ai_generation_jobs)
- `table_columns.unique_table_column_name` (UNIQUE constraint on table_id, name)

---

### Step 2: Implement RecordInsertionService ‚úÖ COMPLETE

**Files Created**:
- `lib/services/enrichment/RecordInsertionService.ts` ‚úÖ
- `lib/services/enrichment/types/insertion.ts` ‚úÖ

**Features Implemented**:
- Field analysis and type inference
- Auto-creation of missing columns
- Hybrid storage (fixed columns + JSONB)
- Metadata tracking (confidence, sources, enrichment method)
- Error handling and progress callbacks

---

### Step 3: Update DataGenerationOrchestrator

**Files to Modify**:
- `lib/services/enrichment/DataGenerationOrchestrator.ts`

**Changes**:
- Add Phase 3 (insertion)
- Integrate RecordInsertionService
- Update progress messages

**Estimated Time**: 2-3 hours

---

### Step 4: Update API Endpoints

**Files to Modify**:
- `app/api/enrichment/generate/route.ts`
- `app/api/enrichment/progress/[sessionId]/route.ts`

**Changes**:
- Add insertion progress messages
- Update response types
- Add error handling

**Estimated Time**: 2 hours

---

### Step 5: Update UI

**Files to Modify**:
- `components/tables/modals/AIEnrichmentModal.tsx`

**Changes**:
- Add insertion stage indicator
- Display column creation messages
- Show per-record insertion progress
- **Add confirmation step before database insertion** (NEW REQUIREMENT)

**Current Behavior**: Auto-inserts to database, then reloads page
**New Behavior**: Show preview ‚Üí User confirms ‚Üí Insert to database

**Estimated Time**: 3-4 hours

---

### Step 6: Testing

**Test Coverage**:
- Unit tests for RecordInsertionService
- Integration tests for full flow
- Manual testing with real data

**Estimated Time**: 4-6 hours

---

## Success Criteria

### Functional Requirements
- ‚úÖ Auto-create columns for new fields
- ‚úÖ Store data in hybrid format (fixed + JSONB)
- ‚úÖ Track confidence scores per field
- ‚úÖ Track enrichment sources and methods
- ‚úÖ Handle all edge cases gracefully
- ‚úÖ Atomic transactions with rollback

### Performance Requirements
- ‚úÖ Insertion time: <10 seconds for 5 records
- ‚úÖ Batch inserts: <2 seconds for 20 records
- ‚úÖ Column creation: <1 second for 10 columns
- ‚úÖ Memory usage: <100MB for 100 records

### Quality Requirements
- ‚úÖ No data loss on errors
- ‚úÖ Proper type inference (>90% accuracy)
- ‚úÖ Full traceability (sources, confidence)
- ‚úÖ Idempotent operations (safe to retry)

---

## Timeline

### Day 1 (8 hours)
- **Morning** (4h): Apply migrations + Implement RecordInsertionService
- **Afternoon** (4h): Update DataGenerationOrchestrator + API endpoints

### Day 2 (8 hours)
- **Morning** (3h): Update UI for insertion progress
- **Afternoon** (5h): Testing + Bug fixes

**Total**: 16 hours (2 days)

---

## UI Confirmation Flow (NEW REQUIREMENT)

### Current Flow (Auto-Insert)
```
Generate Data ‚Üí Show Preview ‚Üí Auto-Insert ‚Üí Reload Page
```

### New Flow (With Confirmation)
```
Generate Data ‚Üí Show Preview ‚Üí User Reviews ‚Üí Click "Add to Table" ‚Üí Insert ‚Üí Success Message
```

### Implementation Details

**Phase States**:
- `form` - Initial form to configure generation
- `generating` - Real-time generation with live preview
- `preview` - Generation complete, awaiting user confirmation (NEW!)
- `inserting` - Inserting to database (NEW!)
- `complete` - Insertion complete

**New Buttons**:
```typescript
// When generation completes (status === "completed")
<div className="flex gap-3">
  <Button
    onClick={handleRestart}
    variant="outline"
    className="rounded-full px-6"
  >
    „ÇÑ„ÇäÁõ¥„Åô
  </Button>
  <Button
    onClick={handleConfirmAndInsert}
    className="bg-[#09090B] text-white rounded-full px-6"
  >
    „ÉÜ„Éº„Éñ„É´„Å´ËøΩÂä† ({liveRecords.length}‰ª∂)
  </Button>
</div>
```

**Insert Handler**:
```typescript
const handleConfirmAndInsert = async () => {
  setPhase('inserting');
  
  try {
    const response = await fetch(`/api/enrichment/insert/${jobId}`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ tableId }),
    });
    
    if (!response.ok) throw new Error('ÊåøÂÖ•„Å´Â§±Êïó„Åó„Åæ„Åó„Åü');
    
    setPhase('complete');
    
    // Show success message for 2 seconds, then close and reload
    setTimeout(() => {
      window.location.reload();
    }, 2000);
    
  } catch (error) {
    alert(error.message);
    setPhase('preview'); // Go back to preview
  }
};
```

**Benefits**:
- User can review data before committing
- User can restart if data quality is poor
- Prevents accidental database pollution
- Better UX control

---

## Next Steps

1. ‚úÖ **Review this plan** with stakeholders
2. ‚úÖ **Apply database migrations** via Supabase MCP (COMPLETE)
3. ‚è≥ **Implement RecordInsertionService**
4. ‚è≥ **Integrate with DataGenerationOrchestrator**
5. ‚è≥ **Create /api/enrichment/insert/[jobId] endpoint** (NEW)
6. ‚è≥ **Update UI with confirmation flow** (NEW)
7. ‚è≥ **Test end-to-end flow**

---

**Document Status**: ‚úÖ Migrations Applied - Ready for Service Implementation  
**Created**: 2024-11-19  
**Migrations Applied**: 2024-11-19  
**UI Requirement Added**: 2024-11-19 (Confirmation step before insert)  
**Related Docs**: AI_DATA_GENERATION_GEMINI_3_REFACTOR.md  
**Remaining Implementation**: 16 hours (Step 3-7)


---

## Implementation Status Update (2024-11-19)

### ‚úÖ Completed Steps

**Step 1: Database Migrations** ‚úÖ
- Added `field_confidence`, `enriched_fields`, `enrichment_method` to `ai_generated_records`
- Added unique constraint to `table_columns` (table_id, name)
- Added `is_ai_generated`, `ai_generation_job_id` to `records`
- All indexes created successfully

**Step 2: RecordInsertionService** ‚úÖ
- Created `lib/services/enrichment/RecordInsertionService.ts`
- Created `lib/services/enrichment/types/insertion.ts`
- Implemented field analysis and type inference
- Implemented auto-column creation with ON CONFLICT handling
- Implemented hybrid storage mapping (fixed + JSONB)
- Implemented metadata building (confidence, sources, enrichment method)
- Added error handling and progress callbacks

**Step 3: API Endpoint** ‚úÖ
- Created `app/api/enrichment/insert/[jobId]/route.ts`
- Implemented authentication and authorization
- Implemented job validation
- Implemented record transformation from ai_generated_records
- Integrated with RecordInsertionService
- Returns detailed insertion results

**Step 4: UI Confirmation Flow** ‚úÖ
- Updated `components/tables/modals/AIEnrichmentModal.tsx`
- Added new phases: `preview`, `inserting`, `complete`
- Added confirmation button: "„ÉÜ„Éº„Éñ„É´„Å´ËøΩÂä† (X‰ª∂)"
- Added "„ÇÑ„ÇäÁõ¥„Åô" button for regeneration
- Added loading states during insertion
- Added success/error feedback
- Auto-reload after successful insertion

### üéØ Ready for Testing

The complete flow is now implemented:
1. User fills form ‚Üí Generate data (Phase 1 + 2)
2. User sees live preview with real-time updates
3. User clicks "„ÉÜ„Éº„Éñ„É´„Å´ËøΩÂä†" to confirm
4. System inserts to database with metadata
5. Success message ‚Üí Auto-reload to show new records

### üìã Testing Checklist

- [ ] Test with existing columns only
- [ ] Test with new columns (auto-creation)
- [ ] Test with mixed existing + new columns
- [ ] Test type inference (email, phone, url, number)
- [ ] Test error handling (database errors)
- [ ] Test confirmation flow (preview ‚Üí insert)
- [ ] Test "„ÇÑ„ÇäÁõ¥„Åô" button
- [ ] Verify metadata storage (confidence, sources)
- [ ] Verify records appear in table after reload
- [ ] Test with 1, 10, 50, 100 records

---

**Implementation Complete**: 2024-11-19  
**Total Time**: ~5 hours  
**Files Created**: 3  
**Files Modified**: 3  
**Status**: Ready for end-to-end testing

---

## Progressive Data Display Implementation (2024-11-19)

### Problem
Users had to wait for ALL records to complete before seeing ANY data in the preview.

### Solution
Added `stage` tracking to show data progressively:

**Database Migration**:
- Added `stage` column to `ai_generation_jobs` table
- Values: `pending`, `knowledge_extraction`, `enrichment`, `completed`, `failed`

**Backend Changes**:
- `DataGenerationOrchestrator.ts`: Updates stage at each phase
  - Start: `knowledge_extraction`
  - After Phase 1: `enrichment`
  - Complete: `completed`
- `progress/[sessionId]/route.ts`: Returns `stage` in response

**Frontend Changes**:
- `AIEnrichmentModal.tsx`: Detects stage changes
  - Fetches preview when stage changes to `enrichment` (Phase 1 complete)
  - Fetches preview when `completedRecords` increases (Phase 2 progress)
  - Shows stage indicator with colored dot

### User Experience Now
1. User clicks generate ‚Üí Sees "Phase 1: Knowledge Extraction" (blue dot)
2. Phase 1 completes ‚Üí Data appears immediately in table (all companies with partial data)
3. Phase 2 starts ‚Üí "Phase 2: Enrichment" (yellow dot)
4. Each company enriched ‚Üí Cells fill in progressively
5. Complete ‚Üí "Completed" (green dot) ‚Üí Confirmation button appears

### Files Modified
- `ai_generation_jobs` table (migration)
- `lib/services/enrichment/DataGenerationOrchestrator.ts`
- `app/api/enrichment/progress/[sessionId]/route.ts`
- `components/tables/modals/AIEnrichmentModal.tsx`


---

## UI Improvements - Row Selection (2024-11-19)

### Features Added

**Shadcn Table with Checkboxes**:
- Replaced plain HTML table with shadcn-styled table
- Added checkbox column for row selection
- "Select All" checkbox in header
- Individual row checkboxes

**Fixed Column Widths**:
- All data columns: 200px fixed width
- Checkbox column: 48px (w-12)
- Index column: 64px (w-16)
- Horizontal scroll enabled for many columns
- Prevents columns from becoming too wide or narrow

**Row Selection State**:
- All rows selected by default
- Users can deselect rows they don't want to insert
- Visual feedback: unselected rows appear with 50% opacity
- Selected count displayed: "X / Y ‰ª∂ÈÅ∏Êäû‰∏≠"

**Insert Only Selected Rows**:
- API updated to accept `selectedIndices` array
- Only selected rows are inserted into database
- Button shows selected count: "„ÉÜ„Éº„Éñ„É´„Å´ËøΩÂä† (X‰ª∂)"
- Button disabled if no rows selected

### User Experience

1. Data appears progressively in table
2. User reviews generated data
3. User unchecks rows they don't want (e.g., low quality data)
4. User clicks "„ÉÜ„Éº„Éñ„É´„Å´ËøΩÂä† (X‰ª∂)" with only selected rows
5. Only selected rows are inserted into database

### Files Modified
- `components/tables/modals/AIEnrichmentModal.tsx`
  - Added `selectedRows` state (Set<number>)
  - Added `handleToggleRow()` and `handleToggleAll()`
  - Updated table to use shadcn Checkbox components
  - Fixed column widths with inline styles
  - Updated insert handler to send `selectedIndices`
- `app/api/enrichment/insert/[jobId]/route.ts`
  - Added `selectedIndices` parameter validation
  - Filter records by selected indices before insertion
